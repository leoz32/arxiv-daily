# üì∞ Daily Papers

**Last update:** 2025-11-06

---
## üîç RAG

### [Using Span Queries to Optimize for Cache and Attention Locality](http://arxiv.org/abs/2511.02749v1)
*Published: 2025-11-04*  
**Authors:** Paul Castro, Nick Mitchell, Nathan Ordonez, Thomas Parnell, Mudhakar Srivatsa, Antoni Viros i Martin

Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are...

### [Relational Deep Dive: Error-Aware Queries Over Unstructured Data](http://arxiv.org/abs/2511.02711v1)
*Published: 2025-11-04*  
**Authors:** Daren Chao, Kaiwen Chen, Naiqing Guan, Nick Koudas

Unstructured data is pervasive, but analytical queries demand structured representations, creating a significant extraction challenge. Existing methods like RAG lack schema awareness and struggle with cross-document alignment, leading to high error rates. We propose ReDD (Relational Deep Dive), a framework that dynamically discovers query-specific schemas, populates relational tables, and ensures ...

### [ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](http://arxiv.org/abs/2511.02415v1)
*Published: 2025-11-04*  
**Authors:** Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang

Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating vi...

### [LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment](http://arxiv.org/abs/2511.02371v1)
*Published: 2025-11-04*  
**Authors:** Rohan Wandre, Yash Gajewar, Namrata Patel, Vivek Dhalkari

Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for grounding large language model outputs in verifiable evidence. However, as modern AI agents transition from static knowledge bases to continuous multimodal streams encompassing text, images, video, and audio, two critical challenges arise: maintaining index freshness without prohibitive re-indexing costs, and preserving ...

### [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](http://arxiv.org/abs/2511.02119v1)
*Published: 2025-11-03*  
**Authors:** Ziheng Geng, Jiachen Liu, Ran Cao, Lu Cheng, Dan M. Frangopol, Minghui Cheng

Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions. Large language models (LLMs) have recently exhibited human-like intelligence across wide-rangin...

### [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](http://arxiv.org/abs/2511.01802v1)
*Published: 2025-11-03*  
**Authors:** Tejas Sarnaik, Manan Shah, Ravi Hegde

Retrieval-Augmented Generation (RAG) has become a robust framework for enhancing Large Language Models (LLMs) with external knowledge. Recent advances in RAG have investigated graph based retrieval for intricate reasoning; however, the influence of prompt design on enhancing the retrieval and reasoning process is still considerably under-examined. In this paper, we present a prompt-driven GraphRAG...

### [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](http://arxiv.org/abs/2511.01668v1)
*Published: 2025-11-03*  
**Authors:** Yueqing Xi, Yifan Bai, Huasen Luo, Weiliang Wen, Hui Liu, Haoliang Li

As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA ag...

### [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](http://arxiv.org/abs/2511.01649v1)
*Published: 2025-11-03*  
**Authors:** Hung-Shin Lee, Chen-Chi Chang, Ching-Yuan Chen, Yun-Hsiang Hsu

This study proposes a cognitive benchmarking framework to evaluate how large language models (LLMs) process and apply culturally specific knowledge. The framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG) to assess model performance across six hierarchical cognitive domains: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. Using a curated Taiwa...

### [A Graph-based RAG for Energy Efficiency Question Answering](http://arxiv.org/abs/2511.01643v1)
*Published: 2025-11-03*  
**Authors:** Riccardo Campi, Nicol√≤ Oreste Pinciroli Vago, Mathyas Giudici, Pablo Barrachina Rodriguez-Guisado, Marco Brambilla, Piero Fraternali

In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate a...

### [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](http://arxiv.org/abs/2511.01581v1)
*Published: 2025-11-03*  
**Authors:** Chengzhang Yu, Zening Lu, Chenyang Zheng, Chiyue Wang, Yiming Zhang, Zhanpeng Jin

Large language models suffer from knowledge staleness and lack of interpretability due to implicit knowledge storage across entangled network parameters, preventing targeted updates and reasoning transparency. We propose ExplicitLM, a novel architecture featuring a million-scale external memory bank storing human-readable knowledge as token sequences, enabling direct inspection and modification. W...

### ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](http://arxiv.org/abs/2511.01454v1)
*Published: 2025-11-03*  
**Authors:** Sergio Torres Aguilar

Translating a morphology-rich, low-resource language like Latin poses significant challenges. This paper introduces a reproducible draft-based refinement pipeline that elevates open-source Large Language Models (LLMs) to a performance level statistically comparable to top-tier proprietary systems. Our method first uses a fine-tuned NLLB-1.3B model to generate a high-quality, structurally faithful ...

### [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](http://arxiv.org/abs/2511.01386v1)
*Published: 2025-11-03*  
**Authors:** Muhammed Yusuf Kartal, Suha Kagan Kose, Korhan Sevin√ß, Burak Aktas

Retrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimize...

### [DeepSpecs: Expert-Level Questions Answering in 5G](http://arxiv.org/abs/2511.01305v1)
*Published: 2025-11-03*  
**Authors:** Aman Ganapathy Manvattira, Yifei Xu, Ziyue Dang, Songwu Lu

5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reas...

### [Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems](http://arxiv.org/abs/2511.01268v1)
*Published: 2025-11-03*  
**Authors:** Minseok Kim, Hankook Lee, Hyungjoon Koo

Large language models (LLMs) are reshaping numerous facets of our daily lives, leading widespread adoption as web-based services. Despite their versatility, LLMs face notable challenges, such as generating hallucinated content and lacking access to up-to-date information. Lately, to address such limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising direction by generating re...

### [Feedback-driven Retrieval-augmented Audio Generation with Large Audio Language Models](http://arxiv.org/abs/2511.01091v1)
*Published: 2025-11-02*  
**Authors:** Junqi Zhao, Chenxing Li, Jinzheng Zhao, Rilin Chen, Dong Yu, Mark D. Plumbley, Wenwu Wang

We propose a general feedback-driven retrieval-augmented generation (RAG) approach that leverages Large Audio Language Models (LALMs) to address the missing or imperfect synthesis of specific sound events in text-to-audio (TTA) generation. Unlike previous RAG-based TTA methods that typically train specialized models from scratch, we utilize LALMs to analyze audio generation outputs, retrieve conce...

---
## üîç Retrieval-Augmented Generation

### [Type II embeddings for $d=6$ Einstein-Maxwell gauged supergravity](http://arxiv.org/abs/2511.02835v1)
*Published: 2025-11-04*  
**Authors:** Niall T. Macpherson, Ricardo Stuardo

Bi-spinor and G-structure methods are used to classify the possible consistent truncations of type II supergravity to $d=6$ Einstein-Maxwell (gauged) supergravity, and its consistent sub-sectors. In the absence of R-symmetry gauging and a tensor multiplet we establish that every supersymmetric Mink$_6$ solution defines an embedding of the $d=6$ theory. Adding a tensor multiplet places restrictions...

### [In Good GRACEs: Principled Teacher Selection for Knowledge Distillation](http://arxiv.org/abs/2511.02833v1)
*Published: 2025-11-04*  
**Authors:** Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Sham Kakade, Surbhi Goel

Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures dist...

### [GeoCrossBench: Cross-Band Generalization for Remote Sensing](http://arxiv.org/abs/2511.02831v1)
*Published: 2025-11-04*  
**Authors:** Hakob Tamazyan, Ani Vanyan, Alvard Barseghyan, Anna Khosrovyan, Evan Shelhamer, Hrant Khachatrian

The number and diversity of remote sensing satellites grows over time, while the vast majority of labeled data comes from older satellites. As the foundation models for Earth observation scale up, the cost of (re-)training to support new satellites grows too, so the generalization capabilities of the models towards new satellites become increasingly important. In this work we introduce GeoCrossBen...

### [From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu](http://arxiv.org/abs/2511.02827v1)
*Published: 2025-11-04*  
**Authors:** Mohamed Almukhtar, Anwar Ghammam, Marouane Kessentini, Hua Ming

In an era shaped by Generative Artificial Intelligence for code generation and the rising adoption of Python-based Machine Learning systems (MLS), software quality has emerged as a major concern. As these systems grow in complexity and importance, a key obstacle lies in understanding exactly how specific code changes affect overall quality-a shortfall aggravated by the lack of quality assessment t...

### [PLUTO-4: Frontier Pathology Foundation Models](http://arxiv.org/abs/2511.02826v2)
*Published: 2025-11-04*  
**Authors:** Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed

Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 f...

### [Neurosymbolic Deep Learning Semantics](http://arxiv.org/abs/2511.02825v1)
*Published: 2025-11-04*  
**Authors:** Artur d'Avila Garcez, Simon Odense

Artificial Intelligence (AI) is a powerful new language of science as evidenced by recent Nobel Prizes in chemistry and physics that recognized contributions to AI applied to those areas. Yet, this new language lacks semantics, which makes AI's scientific discoveries unsatisfactory at best. With the purpose of uncovering new facts but also improving our understanding of the world, AI-based science...

### [Kosmos: An AI Scientist for Autonomous Discovery](http://arxiv.org/abs/2511.02824v2)
*Published: 2025-11-04*  
**Authors:** Ludovico Mitchener, Angela Yiu, Benjamin Chang, Mathieu Bourdenx, Tyler Nadolski, Arvis Sulovari, Eric C. Landsness, Daniel L. Barabasi, Siddharth Narayanan, Nicky Evans, Shriya Reddy, Martha Foiani, Aizad Kamal, Leah P. Shriver, Fang Cao, Asmamaw T. Wassie, Jon M. Laurent, Edwin Melville-Green, Mayk Caldas, Albert Bou, Kaleigh F. Roberts, Sladjana Zagorac, Timothy C. Orr, Miranda E. Orr, Kevin J. Zwezdaryk, Ali E. Ghareeb, Laurie McCoy, Bruna Gomes, Euan A. Ashley, Karen E. Duff, Tonio Buonassisi, Tom Rainforth, Randall J. Bateman, Michael Skarlinski, Samuel G. Rodriques, Michaela M. Hinks, Andrew D. White

Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist tha...

### [Fortifying Distribution Network Nodes Subject to Network-Based Disruptions](http://arxiv.org/abs/2511.02820v1)
*Published: 2025-11-04*  
**Authors:** Pelin Ke≈ürit, Bahar √áavdar, Joseph Geunes

We consider a distribution network for delivering a natural resource or physical good to a set of nodes, each of which serves a set of customers, in which disruptions may occur at one or more nodes. Each node receives flow through a path from a source node, implying that the service at a node is interrupted if one or more nodes on the path from a source node experience a disruption. All network no...

### [From thermal to magnetic driving: spectral diagnostics of simulation-based magnetothermal disc wind models](http://arxiv.org/abs/2511.02811v1)
*Published: 2025-11-04*  
**Authors:** Michael L. Weber, Eleftheria Sarafidou, Christian Rab, Oliver Gressel, Barbara Ercolano

Disc winds driven by thermal and magnetic processes are thought to play a critical role in protoplanetary disc evolution. However, the relative contribution of each mechanism remains uncertain, particularly in light of their observational signatures. We investigate whether spatially resolved emission and synthetic spectral line profiles can distinguish between thermally and magnetically driven win...

### [Topologically Quantized Soliton-Like Pumping using Synthetic Nonlinearity](http://arxiv.org/abs/2511.02806v1)
*Published: 2025-11-04*  
**Authors:** Ankitkumar Maisuriya, Siddhi Mali, Sunil Mittal

The interplay between nonlinear and topological physics has led to intriguing emergent phenomena, such as quantized and fractionally quantized Thouless pumping of solitons dictated by the topological invariants of the underlying band structure. Unlike linear Thouless pumping, which requires excitation of a Wannier function of a uniformly filled band, quantized soliton pumping is observed even with...

### [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](http://arxiv.org/abs/2511.02805v1)
*Published: 2025-11-04*  
**Authors:** Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han

Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemS...

### [Optimal Source Coding of Markov Chains for Real-Time Remote Estimation](http://arxiv.org/abs/2511.02803v1)
*Published: 2025-11-04*  
**Authors:** Ismail Cosandal, Sennur Ulukus

We revisit the source coding problem for a Markov chain under the assumption that the transmission times and how fast the Markov chain transitions its state happen at the same time-scale. Specifically, we assume that the transmission of each bit takes a single time slot, and the Markov chain updates its state in the same time slot. Thus, the length of the codeword assigned to a symbol determines t...

### [Cesam2k20: A code for a new generation of stellar evolution models. I. Description of the code](http://arxiv.org/abs/2511.02801v1)
*Published: 2025-11-04*  
**Authors:** L. Manchon, M. Deal, J. P. C. Marques, Y. Lebreton

We present Cesam2k20, the latest version of the hydrostatic stellar evolution code CESAM originally developed by P. Morel and collaborators. Over the last three decades, it has undergone many improvements and has been extensively tested against other stellar evolution codes before being selected to compute the first-generation grid of stellar models for the PLATO mission. Among all the development...

### [On the origin of exponential operator growth in Hilbert space](http://arxiv.org/abs/2511.02800v1)
*Published: 2025-11-04*  
**Authors:** Vijay Ganesh Sadhasivam, Jan M. Rost, Stuart C. Althorpe

By analysing the growth of Krylov complexity for some simple models, we find that the exponential growth of an operator in Hilbert space can be predicted from the off-diagonal decay of the operator matrix elements in the system eigenbasis. When the decay is exponential or slower, the Krylov complexity grows exponentially; when it is algebraic or slower, the growth rate is maximal. As a result, non...

### [Intercomparison of a High-Resolution Regional Climate Model Ensemble for Catchment-Scale Water Cycle Processes under Human Influence](http://arxiv.org/abs/2511.02799v1)
*Published: 2025-11-04*  
**Authors:** J. L. Roque, F. Da Silva Lopes, J. A. Giles, B. D. Gutknecht, B. Schalge, Y. Zhang, M. Ferro, P. Friederichs, K. Goergen, S. Poll, A. Valmassoi

Understanding regional hydroclimatic variability and its drivers is essential for anticipating the impacts of climate change on water resources and sustainability. Yet, considerable uncertainty remains in the simulation of the coupled land atmosphere water and energy cycles, largely due to structural model limitations, simplified process representations, and insufficient spatial resolution. Within...

---
## üîç Computer Vision

### [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](http://arxiv.org/abs/2511.02834v2)
*Published: 2025-11-04*  
**Authors:** Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh

Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundati...

### [In Good GRACEs: Principled Teacher Selection for Knowledge Distillation](http://arxiv.org/abs/2511.02833v1)
*Published: 2025-11-04*  
**Authors:** Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Sham Kakade, Surbhi Goel

Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures dist...

### [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](http://arxiv.org/abs/2511.02832v1)
*Published: 2025-11-04*  
**Authors:** Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu

Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and ...

### [Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks](http://arxiv.org/abs/2511.02830v1)
*Published: 2025-11-04*  
**Authors:** Dmitrii Pozdeev, Alexey Artemov, Ananta R. Bhattarai, Artem Sevastopolsky

We propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of...

### [PLUTO-4: Frontier Pathology Foundation Models](http://arxiv.org/abs/2511.02826v2)
*Published: 2025-11-04*  
**Authors:** Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed

Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 f...

### [Optimizing AI Agent Attacks With Synthetic Data](http://arxiv.org/abs/2511.02823v1)
*Published: 2025-11-04*  
**Authors:** Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, Joe Benton

As AI deployments become more complex and high-stakes, it becomes increasingly important to be able to estimate their risk. AI control is one framework for doing so. However, good control evaluations require eliciting strong attack policies. This can be challenging in complex agentic environments where compute constraints leave us data-poor. In this work, we show how to optimize attack policies in...

### [Fortifying Distribution Network Nodes Subject to Network-Based Disruptions](http://arxiv.org/abs/2511.02820v1)
*Published: 2025-11-04*  
**Authors:** Pelin Ke≈ürit, Bahar √áavdar, Joseph Geunes

We consider a distribution network for delivering a natural resource or physical good to a set of nodes, each of which serves a set of customers, in which disruptions may occur at one or more nodes. Each node receives flow through a path from a source node, implying that the service at a node is interrupted if one or more nodes on the path from a source node experience a disruption. All network no...

### [Improved lower bounds for the maximum order of an induced acyclic subgraph](http://arxiv.org/abs/2511.02819v1)
*Published: 2025-11-04*  
**Authors:** Shamil Asgarli, Donald Falkenhagen, Kaya Hoshi

Computing the cardinality of a maximum induced acyclic vertex set in a digraph is NP-hard. Since finding an exact solution is computationally difficult, a fruitful approach is to establish high-quality lower bounds that are efficiently computable. We build on the Akbari--Ghodrati--Jabalameli--Saghafian (AGJS) bound for digraphs by adapting refinement techniques used by (a) Selkow and Harant--Mohr ...

### [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](http://arxiv.org/abs/2511.02817v1)
*Published: 2025-11-04*  
**Authors:** Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley

As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one...

### [A Construction of Infinite Families of Self-Orthogonal Quasi-Cyclic Codes Using Constituent Codes.pdf](http://arxiv.org/abs/2511.02813v1)
*Published: 2025-11-04*  
**Authors:** Gustavo Terra Bastos, Angelynn √Ålvarez, Cameron Williams

Quasi-cyclic codes have been recently employed in the constructions of quantum error-correcting codes. In this paper, we propose a construction of infinite families of quasi-cyclic codes which are self-orthogonal with respect to the Euclidean and Hermitian inner products. In particular, their dimension and a lower bound for their minimum distance are computed using their constituent codes defined ...

### [Majorana string simulation of nonequilibrium dynamics in two-dimensional lattice fermion systems](http://arxiv.org/abs/2511.02809v1)
*Published: 2025-11-04*  
**Authors:** Matteo D'Anna, Jannes Nys, Juan Carrasquilla

The study of real-time dynamics of fermions remains one of the last frontiers beyond the reach of classical simulations and is key to our understanding of quantum behavior in chemistry and materials, with implications for quantum technology. Here we introduce a Heisenberg-picture algorithm that propagates observables expressed in a Majorana-string basis using a truncation scheme that preserves Tro...

### [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](http://arxiv.org/abs/2511.02805v1)
*Published: 2025-11-04*  
**Authors:** Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han

Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemS...

### [Cesam2k20: A code for a new generation of stellar evolution models. I. Description of the code](http://arxiv.org/abs/2511.02801v1)
*Published: 2025-11-04*  
**Authors:** L. Manchon, M. Deal, J. P. C. Marques, Y. Lebreton

We present Cesam2k20, the latest version of the hydrostatic stellar evolution code CESAM originally developed by P. Morel and collaborators. Over the last three decades, it has undergone many improvements and has been extensively tested against other stellar evolution codes before being selected to compute the first-generation grid of stellar models for the PLATO mission. Among all the development...

### [Fast, Private, and Protected: Safeguarding Data Privacy and Defending Against Model Poisoning Attacks in Federated Learning](http://arxiv.org/abs/2511.02797v1)
*Published: 2025-11-04*  
**Authors:** Nicolas Riccieri Gardin Assumpcao, Leandro Villas

Federated Learning (FL) is a distributed training paradigm wherein participants collaborate to build a global model while ensuring the privacy of the involved data, which remains stored on participant devices. However, proposals aiming to ensure such privacy also make it challenging to protect against potential attackers seeking to compromise the training outcome. In this context, we present Fast,...

### [Can LLMs subtract numbers?](http://arxiv.org/abs/2511.02795v1)
*Published: 2025-11-04*  
**Authors:** Mayank Jobanputra, Nils Philipp Walter, Maitrey Mehta, Blerta Veseli, Evan Parker Kelly Chapple, Yifan Wang, Sneha Chetani, Ellie Pavlick, Antonio Vergari, Vera Demberg

We present a systematic study of subtraction in large language models (LLMs). While prior benchmarks emphasize addition and multiplication, subtraction has received comparatively little attention despite being structurally distinct as a non-commutative operation. We evaluate eight pretrained LLMs spanning four families on addition and subtraction problems. Our experiments reveal that subtraction a...

---
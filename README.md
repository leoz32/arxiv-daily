# üì∞ Daily Papers

**Last update:** 2025-11-22

---
## üîç RAG

### [Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems](http://arxiv.org/abs/2511.16654v1)
*Published: 2025-11-20*  
**Authors:** Elias Lumer, Alex Cardenas, Matt Melich, Myles Mason, Sara Dieter, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, Roberto Hernandez

Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations...

### [SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction](http://arxiv.org/abs/2511.16635v1)
*Published: 2025-11-20*  
**Authors:** Guolin Huang, Wenting Chen, Jiaqi Yang, Xinheng Lyu, Xiaoling Luo, Sen Yang, Xiaohan Xing, Linlin Shen

Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage expe...

### [CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference](http://arxiv.org/abs/2511.16395v1)
*Published: 2025-11-20*  
**Authors:** Kangwei Xu, Grace Li Zhang, Ulf Schlichtmann, Bing Li

Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to ...

### [ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning](http://arxiv.org/abs/2511.16326v1)
*Published: 2025-11-20*  
**Authors:** Jiawei Zhou, Hang Ding, Haiyun Jiang

Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge...

### [MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering](http://arxiv.org/abs/2511.16283v1)
*Published: 2025-11-20*  
**Authors:** Zhiyuan Li, Haisheng Yu, Guangchuan Guo, Nan Zhou, Jiajun Zhang

Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-I...

### [Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts](http://arxiv.org/abs/2511.16224v1)
*Published: 2025-11-20*  
**Authors:** Francesco Salzano, Simone Scalabrino, Rocco Oliveto, Simone Scalabrino

Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these c...

### [Operon: Incremental Construction of Ragged Data via Named Dimensions](http://arxiv.org/abs/2511.16080v1)
*Published: 2025-11-20*  
**Authors:** Sungbin Moon, Jiho Park, Suyoung Hwang, Donghyun Koh, Seunghyun Moon, Minhyeong Lee

Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookke...

### [CARE-RAG - Clinical Assessment and Reasoning in RAG](http://arxiv.org/abs/2511.15994v1)
*Published: 2025-11-20*  
**Authors:** Deepthi Potluri, Aby Mammen Mathew, Jeffrey B DeWitt, Alexander L. Rasgon, Yide Hao, Junyuan Hong, Ying Ding

Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions...

### [KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy](http://arxiv.org/abs/2511.15974v1)
*Published: 2025-11-20*  
**Authors:** Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou

Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of infection.This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited...

### [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](http://arxiv.org/abs/2511.15435v1)
*Published: 2025-11-19*  
**Authors:** Linyin Luo, Yujuan Ding, Yunshan Ma, Wenqi Fan, Hanjiang Lai

Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work c...

### [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](http://arxiv.org/abs/2511.15355v1)
*Published: 2025-11-19*  
**Authors:** Alexis Correa-Guill√©n, Carlos G√≥mez-Rodr√≠guez, David Vilares

We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and G√≥mez-Rodr√≠guez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish ...

### [Securing AI Agents Against Prompt Injection Attacks](http://arxiv.org/abs/2511.15759v1)
*Published: 2025-11-19*  
**Authors:** Badrinath Ramakrishnan, Akshaya Balaji

Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test c...

### [ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation](http://arxiv.org/abs/2511.15141v1)
*Published: 2025-11-19*  
**Authors:** Sunwoo Kim, Geon Lee, Kyungho Kim, Jaemin Yoo, Kijung Shin

Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and provid...

### [Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics](http://arxiv.org/abs/2511.15752v1)
*Published: 2025-11-19*  
**Authors:** Hanzhi Yan, Qin Lu, Xianqiao Wang, Xiaoming Zhai, Tianming Liu, He Li

While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging bo...

### [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](http://arxiv.org/abs/2511.15074v1)
*Published: 2025-11-19*  
**Authors:** Henrik Bradland, Morten Goodwin, Vladimir I. Zadorozhny, Per-Arne Andersen

The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper i...

---
## üîç Retrieval-Augmented Generation

### [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](http://arxiv.org/abs/2511.16674v1)
*Published: 2025-11-20*  
**Authors:** George Cazenavette, Antonio Torralba, Vincent Sitzmann

The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on larg...

### [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](http://arxiv.org/abs/2511.16672v1)
*Published: 2025-11-20*  
**Authors:** Omkat Thawakar, Shravan Venkatraman, Ritesh Thawkar, Abdelrahman Shaker, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan

Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distil...

### [Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation](http://arxiv.org/abs/2511.16671v1)
*Published: 2025-11-20*  
**Authors:** Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng

Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the fi...

### [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](http://arxiv.org/abs/2511.16669v1)
*Published: 2025-11-20*  
**Authors:** Junhao Cheng, Liang Hou, Xin Tao, Jing Liao

While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new an...

### [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](http://arxiv.org/abs/2511.16668v1)
*Published: 2025-11-20*  
**Authors:** Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You

Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from...

### [SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation](http://arxiv.org/abs/2511.16666v1)
*Published: 2025-11-20*  
**Authors:** Zhenyuan Qin, Xincheng Shuai, Henghui Ding

Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, f...

### [Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter](http://arxiv.org/abs/2511.16665v1)
*Published: 2025-11-20*  
**Authors:** Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han

The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very lon...

### [Worldline Localization](http://arxiv.org/abs/2511.16663v1)
*Published: 2025-11-20*  
**Authors:** Changha Choi, Leon A. Takhtajan

We show that two elementary worldline path integrals-the thermal partition function of the harmonic oscillator and the one-loop effective action of scalar QED in a constant field strength-exhibit a natural form of supersymmetric localization. The mechanism relies on hidden fermionic symmetries of the worldline BRST formulation, rather than on standard BRST structure or physical supersymmetry. Thes...

### [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](http://arxiv.org/abs/2511.16662v1)
*Published: 2025-11-20*  
**Authors:** Eddie Pokming Sheung, Qihao Liu, Wufei Ma, Prakhar Kaushik, Jianwen Xie, Alan Yuille

With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high compu...

### [PartUV: Part-Based UV Unwrapping of 3D Meshes](http://arxiv.org/abs/2511.16659v1)
*Published: 2025-11-20*  
**Authors:** Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, Minghua Liu

UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing ar...

### [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](http://arxiv.org/abs/2511.16657v1)
*Published: 2025-11-20*  
**Authors:** Juan C. King, Jose M. Amigo

This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) co...

### [Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems](http://arxiv.org/abs/2511.16654v1)
*Published: 2025-11-20*  
**Authors:** Elias Lumer, Alex Cardenas, Matt Melich, Myles Mason, Sara Dieter, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, Roberto Hernandez

Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations...

### [Evolution Strategies at the Hyperscale](http://arxiv.org/abs/2511.16652v1)
*Published: 2025-11-20*  
**Authors:** Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio Le√≥n Villares, Anya Sims, Dylan Cope, Jarek Liesen, Lukas Seier, Theo Wolf, Uljad Berdica, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster

We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling...

### [InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](http://arxiv.org/abs/2511.16651v1)
*Published: 2025-11-20*  
**Authors:** Yang Tian, Yuyin Yang, Yiman Xie, Zetao Cai, Xu Shi, Ning Gao, Hangxu Liu, Xuekun Jiang, Zherui Qiu, Feng Yuan, Yaping Li, Ping Wang, Junhao Cai, Jia Zeng, Hao Dong, Jiangmiao Pang

Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strong...

### [The metric theory of small gaps for a sequence of real numbers](http://arxiv.org/abs/2511.16647v1)
*Published: 2025-11-20*  
**Authors:** Jewel Mahajan

Let $(a_n)_{n \geq 1}$ be a sequence of distinct positive integers. The metric theory of minimal gaps for the sequence $\{Œ±a_n \text{ mod }1, 1\leq n \leq N\}$ as $N \to \infty$ was initiated by Rudnick, who established that the minimal gap admits an asymptotic upper bound expressible in terms of the additive energy of $\{a_1,\ldots,a_N\}$ for almost every $Œ±$. Later, Aistleitner, El-Baz, and Muns...

---
## üîç Computer Vision

### [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](http://arxiv.org/abs/2511.16674v1)
*Published: 2025-11-20*  
**Authors:** George Cazenavette, Antonio Torralba, Vincent Sitzmann

The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on larg...

### [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](http://arxiv.org/abs/2511.16672v1)
*Published: 2025-11-20*  
**Authors:** Omkat Thawakar, Shravan Venkatraman, Ritesh Thawkar, Abdelrahman Shaker, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan

Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distil...

### [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](http://arxiv.org/abs/2511.16673v1)
*Published: 2025-11-20*  
**Authors:** Jing Wen, Alexander G. Schwing, Shenlong Wang

We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome th...

### [Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation](http://arxiv.org/abs/2511.16671v1)
*Published: 2025-11-20*  
**Authors:** Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng

Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the fi...

### [Learning to Think Fast and Slow for Visual Language Models](http://arxiv.org/abs/2511.16670v1)
*Published: 2025-11-20*  
**Authors:** Chenyu Lin, Cheng Chi, Jinlin Wu, Sharon Li, Kaiyang Zhou

When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models ...

### [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](http://arxiv.org/abs/2511.16669v1)
*Published: 2025-11-20*  
**Authors:** Junhao Cheng, Liang Hou, Xin Tao, Jing Liao

While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new an...

### [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](http://arxiv.org/abs/2511.16668v1)
*Published: 2025-11-20*  
**Authors:** Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You

Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from...

### [SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation](http://arxiv.org/abs/2511.16666v1)
*Published: 2025-11-20*  
**Authors:** Zhenyuan Qin, Xincheng Shuai, Henghui Ding

Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, f...

### [Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter](http://arxiv.org/abs/2511.16665v1)
*Published: 2025-11-20*  
**Authors:** Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han

The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very lon...

### [Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs](http://arxiv.org/abs/2511.16664v1)
*Published: 2025-11-20*  
**Authors:** Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Ruisi Cai, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov

Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this p...

### [Worldline Localization](http://arxiv.org/abs/2511.16663v1)
*Published: 2025-11-20*  
**Authors:** Changha Choi, Leon A. Takhtajan

We show that two elementary worldline path integrals-the thermal partition function of the harmonic oscillator and the one-loop effective action of scalar QED in a constant field strength-exhibit a natural form of supersymmetric localization. The mechanism relies on hidden fermionic symmetries of the worldline BRST formulation, rather than on standard BRST structure or physical supersymmetry. Thes...

### [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](http://arxiv.org/abs/2511.16662v1)
*Published: 2025-11-20*  
**Authors:** Eddie Pokming Sheung, Qihao Liu, Wufei Ma, Prakhar Kaushik, Jianwen Xie, Alan Yuille

With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high compu...

### [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](http://arxiv.org/abs/2511.16661v1)
*Published: 2025-11-20*  
**Authors:** Irmak Guzey, Haozhi Qi, Julen Urain, Changhao Wang, Jessica Yin, Krishna Bodduluri, Mike Lambeta, Lerrel Pinto, Akshara Rai, Jitendra Malik, Tingfan Wu, Akash Sharma, Homanga Bharadhwaj

Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been ...

### [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](http://arxiv.org/abs/2511.16660v1)
*Published: 2025-11-20*  
**Authors:** Priyanka Kargupta, Shuyue Stella Li, Haocheng Wang, Jinu Lee, Shan Chen, Orevaoghene Ahia, Dean Light, Thomas L. Griffiths, Max Kleiman-Weiner, Jiawei Han, Asli Celikyilmaz, Yulia Tsvetkov

Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their ...

### [PartUV: Part-Based UV Unwrapping of 3D Meshes](http://arxiv.org/abs/2511.16659v1)
*Published: 2025-11-20*  
**Authors:** Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, Minghua Liu

UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing ar...

---
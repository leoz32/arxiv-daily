{
  "meta": {
    "last_update": "2025-11-16T03:22:10Z",
    "keywords": [
      "RAG",
      "Retrieval-Augmented Generation",
      "Computer Vision"
    ],
    "counts": {
      "RAG": 15,
      "Retrieval-Augmented Generation": 15,
      "Computer Vision": 15
    }
  },
  "data": {
    "RAG": [
      {
        "title": "Convomem Benchmark: Why Your First 150 Conversations Don't Need RAG",
        "authors": [
          "Egor Pakhomov",
          "Erik Nijkamp",
          "Caiming Xiong"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10523v1",
        "summary": "We introduce a comprehensive benchmark for conversational memory evaluation containing 75,336 question-answer pairs across diverse categories including user facts, assistant recall, abstention, preferences, temporal changes, and implicit connections. While existing benchmarks have advanced the field, our work addresses fundamental challenges in statistical power, data generation consistency, and e..."
      },
      {
        "title": "TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs",
        "authors": [
          "Shuyi Liu",
          "Yuming Shang",
          "Xi Zhang"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10375v1",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external..."
      },
      {
        "title": "Local Hybrid Retrieval-Augmented Document QA",
        "authors": [
          "Paolo Astrino"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10297v1",
        "summary": "Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entir..."
      },
      {
        "title": "RAGFort: Dual-Path Defense Against Proprietary Knowledge Base Extraction in Retrieval-Augmented Generation",
        "authors": [
          "Qinfeng Li",
          "Miao Pan",
          "Ke Xiong",
          "Ge Su",
          "Zhiqiang Shen",
          "Yan Liu",
          "Bing Sun",
          "Hao Peng",
          "Xuhong Zhang"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10128v1",
        "summary": "Retrieval-Augmented Generation (RAG) systems deployed over proprietary knowledge bases face growing threats from reconstruction attacks that aggregate model responses to replicate knowledge bases. Such attacks exploit both intra-class and inter-class paths, progressively extracting fine-grained knowledge within topics and diffusing it across semantically related ones, thereby enabling comprehensiv..."
      },
      {
        "title": "fastbmRAG: A Fast Graph-Based RAG Framework for Efficient Processing of Large-Scale Biomedical Literature",
        "authors": [
          "Guofeng Meng",
          "Li Shen",
          "Qiuyan Zhong",
          "Wei Wang",
          "Haizhou Zhang",
          "Xiaozhen Wang"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10014v1",
        "summary": "Large language models (LLMs) are rapidly transforming various domains, including biomedicine and healthcare, and demonstrate remarkable potential from scientific research to new drug discovery. Graph-based retrieval-augmented generation (RAG) systems, as a useful application of LLMs, can improve contextual reasoning through structured entity and relationship identification from long-context knowle..."
      },
      {
        "title": "Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation",
        "authors": [
          "Bo Li",
          "Zhenghua Xu",
          "Rui Xie"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.09984v1",
        "summary": "Multilingual Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to perform knowledge-intensive tasks in multilingual settings by leveraging retrieved documents as external evidence. However, when the retrieved evidence differs in language from the user query and in-context exemplars, the model often exhibits language drift by generating responses in an unintended language. T..."
      },
      {
        "title": "Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG",
        "authors": [
          "Bo Li",
          "Tian Tian",
          "Zhenghua Xu",
          "Hao Cheng",
          "Shikun Zhang",
          "Wei Ye"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.09980v1",
        "summary": "Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already ..."
      },
      {
        "title": "REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering",
        "authors": [
          "Yijie Zhu",
          "Haojie Zhou",
          "Wanting Hong",
          "Tailin Liu",
          "Ning Wang"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.09966v1",
        "summary": "Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcome..."
      },
      {
        "title": "MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection",
        "authors": [
          "Pritish Sahu",
          "Anirudh Som",
          "Dimitra Vergyri",
          "Ajay Divakaran"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.09918v1",
        "summary": "Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the ..."
      },
      {
        "title": "Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM",
        "authors": [
          "Neo Wang",
          "Sonit Singh"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.09831v1",
        "summary": "The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary c..."
      },
      {
        "title": "TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG",
        "authors": [
          "Yufeng Wang",
          "Lu wei",
          "Haibin Ling"
        ],
        "published": "2025-11-12",
        "link": "http://arxiv.org/abs/2511.09803v1",
        "summary": "Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft's prefix logits, TARG computes lightweight uncertainty scores: mean token..."
      },
      {
        "title": "Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs",
        "authors": [
          "Etienne Dallaire"
        ],
        "published": "2025-11-12",
        "link": "http://arxiv.org/abs/2511.09545v1",
        "summary": "This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build a..."
      },
      {
        "title": "Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation",
        "authors": [
          "Joschka Kersting",
          "Michael Rummel",
          "Gesa Benndorf"
        ],
        "published": "2025-11-12",
        "link": "http://arxiv.org/abs/2511.09122v1",
        "summary": "Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust c..."
      },
      {
        "title": "Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning",
        "authors": [
          "Wenda Wei",
          "Yu-An Liu",
          "Ruqing Zhang",
          "Jiafeng Guo",
          "Lixin Su",
          "Shuaiqiang Wang",
          "Dawei Yin",
          "Maarten de Rijke",
          "Xueqi Cheng"
        ],
        "published": "2025-11-12",
        "link": "http://arxiv.org/abs/2511.09109v2",
        "summary": "Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit gui..."
      },
      {
        "title": "AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines",
        "authors": [
          "Alvin Chauhan"
        ],
        "published": "2025-11-12",
        "link": "http://arxiv.org/abs/2511.09005v1",
        "summary": "Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of ..."
      }
    ],
    "Retrieval-Augmented Generation": [
      {
        "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
        "authors": [
          "Jiahao Wang",
          "Weiye Xu",
          "Aijun Yang",
          "Wengang Zhou",
          "Lewei Lu",
          "Houqiang Li",
          "Xiaohua Wang",
          "Jinguo Zhu"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10648v1",
        "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of..."
      },
      {
        "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
        "authors": [
          "Haotong Lin",
          "Sili Chen",
          "Junhao Liew",
          "Donny Y. Chen",
          "Zhenyu Li",
          "Guang Shi",
          "Jiashi Feng",
          "Bingyi Kang"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10647v1",
        "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates..."
      },
      {
        "title": "Ordinary lattice defects as probes of topology",
        "authors": [
          "Aiden J. Mains",
          "Jia-Xin Zhong",
          "Yun Jing",
          "Bitan Roy"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10646v1",
        "summary": "In addition to topological lattice defects such as dislocations and disclinations, crystals are also accompanied by unavoidable ordinary defects, devoid of any non-trivial geometry or topology, among which vacancies, Schottky defects, substitutions, interstitials, and Frenkel pairs are the most common. In this work, we demonstrate that these ubiquitous ordinary lattice defects, though topologicall..."
      },
      {
        "title": "Black-Box On-Policy Distillation of Large Language Models",
        "authors": [
          "Tianzhu Ye",
          "Li Dong",
          "Zewen Chi",
          "Xun Wu",
          "Shaohan Huang",
          "Furu Wei"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10643v1",
        "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its re..."
      },
      {
        "title": "Flexible Simulation Based Inference for Galaxy Photometric Fitting with Synthesizer",
        "authors": [
          "Thomas Harvey",
          "Christopher C. Lovell",
          "Sophie Newman",
          "Christopher J. Conselice",
          "Duncan Austin",
          "Aswin P. Vijayan",
          "Stephen M. Wilkins",
          "Vadim Rusakov",
          "Qiong Li",
          "Nathan Adams",
          "Kai Magdwick",
          "Matthew Ho"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10640v1",
        "summary": "We introduce Synference, a new, flexible Python framework for galaxy SED fitting using simulation-based inference (SBI). Synference leverages the Synthesizer package for flexible forward-modelling of galaxy SEDs and integrates the LtU-ILI package to ensure best practices in model training and validation. In this work we demonstrate Synference by training a neural posterior estimator on $10^6$ simu..."
      },
      {
        "title": "Emergent spin order and steady-state superradiance in one-dimensional baths",
        "authors": [
          "Silvia Cardenas-Lopez",
          "Edgar Guardiola-Navarrete",
          "Ana Asenjo-Garcia"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10638v1",
        "summary": "Spontaneous collective decay in driven atomic ensembles can generate coherence far from equilibrium, as illustrated by superradiant lasers where decay into a single-mode cavity synchronizes atomic phases into a macroscopic dipole and yields superradiant emission of light with an ultranarrow spectrum. Whether similar ordering persists in multimode reservoirs with propagation and competing collectiv..."
      },
      {
        "title": "Asymptotic Simplicity and Scattering in General Relativity from Quantum Field Theory",
        "authors": [
          "Stefano De Angelis",
          "Aidan Herderschee",
          "Radu Roiban",
          "Fei Teng"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10637v1",
        "summary": "We investigate the fate of asymptotic simplicity in physically relevant settings of compact-object scattering. Using the stress tensor of a two-body system as a source, we compute the spacetime metric in General Relativity at finite observer distance in an asymptotic expansion. To do so, we relate the metric to the final-state graviton one-point function in momentum space, which is computed using ..."
      },
      {
        "title": "Eigenfunctions of deformed Schrödinger equations",
        "authors": [
          "Matijn François",
          "Alba Grassi",
          "Tommaso Pedroni"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10636v1",
        "summary": "We study the spectral problems associated with the finite-difference operators $H_N = 2 \\cosh(p) + V_N(x)$, where $V_N(x)$ is an arbitrary polynomial potential of degree $N$. These systems can be regarded as a solvable deformation of the standard Schrödinger operators $p^2 + V_N(x)$, and they arise naturally from the quantization of the Seiberg-Witten curve of four-dimensional, $\\mathcal{N} = 2$, ..."
      },
      {
        "title": "Non-stationary noise in gravitational wave analyses: The wavelet domain noise covariance matrix",
        "authors": [
          "Neil J. Cornish"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10632v1",
        "summary": "Gravitational wave detectors produce time series of the gravitational wave strain co-added with instrument noise. For evenly sampled data, such as from laser interferometers, it has been traditional to Fourier transform the data and perform analyses in the frequency domain. The motivation being that the Fourier domain noise covariance matrix will be diagonal if the noise properties are constant in..."
      },
      {
        "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
        "authors": [
          "Aleksandr Razin",
          "Danil Kazantsev",
          "Ilya Makarov"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10629v1",
        "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE..."
      },
      {
        "title": "Instella: Fully Open Language Models with Stellar Performance",
        "authors": [
          "Jiang Liu",
          "Jialian Wu",
          "Xiaodong Yu",
          "Yusheng Su",
          "Prakamya Mishra",
          "Gowtham Ramesh",
          "Sudhanshu Ranjan",
          "Chaitanya Manem",
          "Ximeng Sun",
          "Ze Wang",
          "Pratik Prabhanjan Brahma",
          "Zicheng Liu",
          "Emad Barsoum"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10628v1",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Ins..."
      },
      {
        "title": "Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity",
        "authors": [
          "Ilyas Fatkhullin",
          "Niao He",
          "Guanghui Lan",
          "Florian Wolf"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10626v1",
        "summary": "Constrained non-convex optimization is fundamentally challenging, as global solutions are generally intractable and constraint qualifications may not hold. However, in many applications, including safe policy optimization in control and reinforcement learning, such problems possess hidden convexity, meaning they can be reformulated as convex programs via a nonlinear invertible transformation. Typi..."
      },
      {
        "title": "Verification of Sequential Convex Programming for Parametric Non-convex Optimization",
        "authors": [
          "Rajiv Sambharya",
          "Nikolai Matni",
          "George Pappas"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10622v1",
        "summary": "We introduce a verification framework to exactly verify the worst-case performance of sequential convex programming (SCP) algorithms for parametric non-convex optimization. The verification problem is formulated as an optimization problem that maximizes a performance metric (e.g., the suboptimality after a given number of iterations) over parameters constrained to be in a parameter set and iterate..."
      },
      {
        "title": "Know Your Limits: Entropy Estimation Modeling for Compression and Generalization",
        "authors": [
          "Benjamin L. Badger",
          "Matthew Neligeorge"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10618v1",
        "summary": "Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language en..."
      },
      {
        "title": "Dark Matter from Holography",
        "authors": [
          "Oem Trivedi",
          "Robert J. Scherrer"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10617v1",
        "summary": "Previous studies have examined the holographic principle as a means of producing dark energy. Here we propose instead the possibility of holographic dark matter. In this case, dark matter does not arise in the framework of particle physics but is derived from the infrared cutoff set by the horizon scale. Using the Ricci cutoff, and a universe containing only baryons and radiation, we can account f..."
      }
    ],
    "Computer Vision": [
      {
        "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
        "authors": [
          "Jiahao Wang",
          "Weiye Xu",
          "Aijun Yang",
          "Wengang Zhou",
          "Lewei Lu",
          "Houqiang Li",
          "Xiaohua Wang",
          "Jinguo Zhu"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10648v1",
        "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of..."
      },
      {
        "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
        "authors": [
          "Haotong Lin",
          "Sili Chen",
          "Junhao Liew",
          "Donny Y. Chen",
          "Zhenyu Li",
          "Guang Shi",
          "Jiashi Feng",
          "Bingyi Kang"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10647v1",
        "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates..."
      },
      {
        "title": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference",
        "authors": [
          "Yesheng Liang",
          "Haisheng Chen",
          "Song Han",
          "Zhijian Liu"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10645v1",
        "summary": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chain..."
      },
      {
        "title": "Black-Box On-Policy Distillation of Large Language Models",
        "authors": [
          "Tianzhu Ye",
          "Li Dong",
          "Zewen Chi",
          "Xun Wu",
          "Shaohan Huang",
          "Furu Wei"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10643v1",
        "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its re..."
      },
      {
        "title": "Asymptotic Simplicity and Scattering in General Relativity from Quantum Field Theory",
        "authors": [
          "Stefano De Angelis",
          "Aidan Herderschee",
          "Radu Roiban",
          "Fei Teng"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10637v1",
        "summary": "We investigate the fate of asymptotic simplicity in physically relevant settings of compact-object scattering. Using the stress tensor of a two-body system as a source, we compute the spacetime metric in General Relativity at finite observer distance in an asymptotic expansion. To do so, we relate the metric to the final-state graviton one-point function in momentum space, which is computed using ..."
      },
      {
        "title": "Robot Crash Course: Learning Soft and Stylized Falling",
        "authors": [
          "Pascal Strauch",
          "David Müller",
          "Sammy Christen",
          "Agon Serifi",
          "Ruben Grandia",
          "Espen Knoop",
          "Moritz Bächer"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10635v1",
        "summary": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward f..."
      },
      {
        "title": "Impacts of Decoder Latency on Utility-Scale Quantum Computer Architectures",
        "authors": [
          "Abdullah Khalid",
          "Allyson Silva",
          "Gebremedhin A. Dagnew",
          "Tom Dvir",
          "Oded Wertheim",
          "Motty Gruda",
          "Xiangzhou Kong",
          "Mia Kramer",
          "Zak Webb",
          "Artur Scherer",
          "Masoud Mohseni",
          "Yonatan Cohen",
          "Pooya Ronagh"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10633v1",
        "summary": "The speed of a fault-tolerant quantum computer is dictated by the reaction time of its classical electronics, that is, the total time required by decoders and controllers to determine the outcome of a logical measurement and execute subsequent conditional logical operations. Despite its importance, the reaction time and its impact on the design of the logical microarchitecture of a quantum compute..."
      },
      {
        "title": "A Bayesian Perspective on Evidence for Evolving Dark Energy",
        "authors": [
          "Dily Duan Yi Ong",
          "David Yallup",
          "Will Handley"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10631v1",
        "summary": "The DESI collaboration reports a significant preference for a dynamic dark energy model ($w_0w_a$CDM) over the cosmological constant ($Λ$CDM) when their data are combined with other frontier cosmological probes. We present a direct Bayesian model comparison using nested sampling to compute the Bayesian evidence, revealing a contrasting conclusion: for the key combination of the DESI DR2 BAO and th..."
      },
      {
        "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
        "authors": [
          "Aleksandr Razin",
          "Danil Kazantsev",
          "Ilya Makarov"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10629v1",
        "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE..."
      },
      {
        "title": "Instella: Fully Open Language Models with Stellar Performance",
        "authors": [
          "Jiang Liu",
          "Jialian Wu",
          "Xiaodong Yu",
          "Yusheng Su",
          "Prakamya Mishra",
          "Gowtham Ramesh",
          "Sudhanshu Ranjan",
          "Chaitanya Manem",
          "Ximeng Sun",
          "Ze Wang",
          "Pratik Prabhanjan Brahma",
          "Zicheng Liu",
          "Emad Barsoum"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10628v1",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Ins..."
      },
      {
        "title": "Querying Labeled Time Series Data with Scenario Programs",
        "authors": [
          "Edward Kim",
          "Devan Shanker",
          "Varun Bharadwaj",
          "Hongbeen Park",
          "Jinkyu Kim",
          "Hazem Torfah",
          "Daniel J Fremont",
          "Sanjit A Seshia"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10627v1",
        "summary": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world..."
      },
      {
        "title": "Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity",
        "authors": [
          "Ilyas Fatkhullin",
          "Niao He",
          "Guanghui Lan",
          "Florian Wolf"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10626v1",
        "summary": "Constrained non-convex optimization is fundamentally challenging, as global solutions are generally intractable and constraint qualifications may not hold. However, in many applications, including safe policy optimization in control and reinforcement learning, such problems possess hidden convexity, meaning they can be reformulated as convex programs via a nonlinear invertible transformation. Typi..."
      },
      {
        "title": "Model-oriented Graph Distances via Partially Ordered Sets",
        "authors": [
          "Armeen Taeb",
          "F. Richard Guo",
          "Leonard Henckel"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10625v1",
        "summary": "A well-defined distance on the parameter space is key to evaluating estimators, ensuring consistency, and building confidence sets. While there are typically standard distances to adopt in a continuous space, this is not the case for combinatorial parameters such as graphs that represent statistical models. Existing proposals like the structural Hamming distance are defined on the graphs rather th..."
      },
      {
        "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
        "authors": [
          "Haizhou Shi",
          "Ye Liu",
          "Bo Pang",
          "Zeyu Leo Liu",
          "Hao Wang",
          "Silvio Savarese",
          "Caiming Xiong",
          "Yingbo Zhou",
          "Semih Yavuz"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10621v1",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model respo..."
      },
      {
        "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem",
        "authors": [
          "Avrim Blum",
          "Marten Garicano",
          "Kavya Ravichandran",
          "Dravyansh Sharma"
        ],
        "published": "2025-11-13",
        "link": "http://arxiv.org/abs/2511.10619v1",
        "summary": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms fo..."
      }
    ]
  }
}